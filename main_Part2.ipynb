{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol4zFkSbjgXF"
   },
   "source": [
    "# PART 2. Sentence retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GlliDsgXjisj"
   },
   "outputs": [],
   "source": [
    "# built-in libs\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "from itertools import combinations\n",
    "# third-party libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset, Dataset\n",
    "\n",
    "# local libs\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J3BBLE3_hlPi"
   },
   "outputs": [],
   "source": [
    "SEED = 20\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
    "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
    "DOC_DATA = load_json(\"data/train_doc5.jsonl\")\n",
    "\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "_y = [LABEL2ID[data[\"label\"]] for data in TRAIN_DATA]\n",
    "# GT means Ground Truth\n",
    "TRAIN_GT, DEV_GT = train_test_split(\n",
    "    DOC_DATA,\n",
    "    test_size=0.05,\n",
    "    random_state=SEED,\n",
    "    shuffle=True,\n",
    "    stratify=_y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mapping_path = Path(f\"data/part2_mapping.json\")\n",
    "if mapping_path.exists():\n",
    "    mapping = json.load( open( \"data/part2_mapping.json\" ) )\n",
    "else:\n",
    "    wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "    mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "    json.dump( mapping, open( \"data/part2_mapping.json\", 'w' ) )\n",
    "    del wiki_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence_macro_precision(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate precision for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of precision)\n",
    "        [2]: retrieved (denominator of precision)\n",
    "    \"\"\"\n",
    "    this_precision = 0.0\n",
    "    this_precision_hits = 0.0\n",
    "\n",
    "    # Return 0, 0 if label is not enough info since not enough info does not\n",
    "    # contain any evidence.\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # e[2] is the page title, e[3] is the sentence index\n",
    "        all_evi = [[e[2], e[3]]\n",
    "                   for eg in instance[\"evidence\"]\n",
    "                   for e in eg\n",
    "                   if e[3] is not None]\n",
    "        claim = instance[\"claim\"]\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for prediction in predicted_evidence:\n",
    "            if prediction in all_evi:\n",
    "                this_precision += 1.0\n",
    "            this_precision_hits += 1.0\n",
    "\n",
    "        return (this_precision /\n",
    "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
    "\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate recall for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence_macro_recall(\n",
    "    instance: Dict,\n",
    "    top_rows: pd.DataFrame,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculate recall for sentence retrieval\n",
    "    This function is modified from fever-scorer.\n",
    "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
    "\n",
    "    Args:\n",
    "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
    "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
    "\n",
    "        IMPORTANT!!!\n",
    "        instance (dict) should have the key of `evidence`.\n",
    "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]:\n",
    "        [1]: relevant and retrieved (numerator of recall)\n",
    "        [2]: relevant (denominator of recall)\n",
    "    \"\"\"\n",
    "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
    "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
    "        # If there's no evidence to predict, return 1\n",
    "        if len(instance[\"evidence\"]) == 0 or all(\n",
    "            [len(eg) == 0 for eg in instance]):\n",
    "            return 1.0, 1.0\n",
    "\n",
    "        claim = instance[\"claim\"]\n",
    "\n",
    "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
    "                                      claim][\"predicted_evidence\"].tolist()\n",
    "\n",
    "        for evidence_group in instance[\"evidence\"]:\n",
    "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
    "            if all([item in predicted_evidence for item in evidence]):\n",
    "                # We only want to score complete groups of evidence. Incomplete\n",
    "                # groups are worthless.\n",
    "                return 1.0, 1.0\n",
    "        return 0.0, 1.0\n",
    "    return 0.0, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the scores of sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    probs: np.ndarray,\n",
    "    df_evidences: pd.DataFrame,\n",
    "    ground_truths: pd.DataFrame,\n",
    "    top_n: int = 5,\n",
    "    cal_scores: bool = True,\n",
    "    save_name: str = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Calculate the scores of sentence retrieval\n",
    "\n",
    "    Args:\n",
    "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
    "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
    "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
    "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: F1 score, precision, and recall\n",
    "    \"\"\"\n",
    "    df_evidences[\"prob\"] = probs\n",
    "    \n",
    "    # 對每個Claim的所有句子中，選出5個最有可能是相關句的句子出來。  (每句話在這裡已經被標上了prob，其實就是對每個claim取前5高的prob)\n",
    "    top_rows = (\n",
    "        df_evidences.groupby(\"claim\").apply(\n",
    "        lambda x: x.nlargest(top_n, \"prob\"))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if cal_scores:\n",
    "        macro_precision = 0\n",
    "        macro_precision_hits = 0\n",
    "        macro_recall = 0\n",
    "        macro_recall_hits = 0\n",
    "\n",
    "        for i, instance in enumerate(ground_truths):\n",
    "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
    "            macro_precision += macro_prec[0]\n",
    "            macro_precision_hits += macro_prec[1]\n",
    "\n",
    "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
    "            macro_recall += macro_rec[0]\n",
    "            macro_recall_hits += macro_rec[1]\n",
    "\n",
    "        pr = (macro_precision /\n",
    "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
    "        rec = (macro_recall /\n",
    "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
    "        f1 = 2.0 * pr * rec / (pr + rec)\n",
    "\n",
    "    if save_name is not None:\n",
    "        # write doc7_sent5 file\n",
    "        with open(f\"data/{save_name}\", \"w\") as f:\n",
    "            for instance in ground_truths:\n",
    "                claim = instance[\"claim\"]\n",
    "                predicted_evidence = top_rows[\n",
    "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
    "                instance[\"predicted_evidence\"] = predicted_evidence\n",
    "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if cal_scores:\n",
    "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference script to get probabilites for the candidate evidence sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_probs(\n",
    "    model: nn.Module,\n",
    "    dataloader: Dataset,\n",
    "    device: torch.device,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Inference script to get probabilites for the candidate evidence sentences\n",
    "\n",
    "    Args:\n",
    "        model: the one from HuggingFace Transformers\n",
    "        dataloader: devset or testset in torch dataloader\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: probabilites of the candidate evidence sentences\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probs.extend(torch.softmax(logits, dim=1)[:, 1].tolist())\n",
    "    \n",
    "    return np.array(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentRetrievalBERTDataset(BERTDataset):\n",
    "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        sentA = item[\"claim\"]\n",
    "        sentB = item[\"text\"]\n",
    "\n",
    "        # claim [SEP] text\n",
    "        concat = self.tokenizer(\n",
    "            sentA,\n",
    "            sentB,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "#         print(sentA , sentB ,concat)\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(item[\"label\"])\n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for sentence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gpvXpFwXszfv"
   },
   "outputs": [],
   "source": [
    "def pair_with_wiki_sentences(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    negative_ratio: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating train sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # my positive\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "        evidence_sets = df[\"evidence\"].iloc[i]\n",
    "        for evidence_set in evidence_sets:\n",
    "            sents = []\n",
    "            for evidence in evidence_set:\n",
    "                # evidence[2] is the page title\n",
    "                page = evidence[2].replace(\" \", \"_\")\n",
    "                # the only page with weird name\n",
    "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "                    continue\n",
    "                # evidence[3] is in form of int however, mapping requires str\n",
    "                sent_idx = str(evidence[3])\n",
    "                claims.append(claim)\n",
    "                sentences.append(mapping[page][sent_idx])\n",
    "                labels.append(1)\n",
    "\n",
    "\n",
    "    # positive\n",
    "#     for i in range(len(df)):\n",
    "#         if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "#             continue\n",
    "\n",
    "#         claim = df[\"claim\"].iloc[i]\n",
    "#         evidence_sets = df[\"evidence\"].iloc[i]\n",
    "#         for evidence_set in evidence_sets:\n",
    "#             sents = []\n",
    "#             for evidence in evidence_set:\n",
    "#                 # evidence[2] is the page title\n",
    "#                 page = evidence[2].replace(\" \", \"_\")\n",
    "#                 # the only page with weird name\n",
    "#                 if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
    "#                     continue\n",
    "#                 # evidence[3] is in form of int however, mapping requires str\n",
    "#                 sent_idx = str(evidence[3])\n",
    "#                 sents.append(mapping[page][sent_idx])\n",
    "\n",
    "#             whole_evidence = \" \".join(sents) #將所有證據句以空格分隔的方式串起來\n",
    "\n",
    "#             claims.append(claim) \n",
    "#             sentences.append(whole_evidence) # 可以發現，這裡會將串起來的證據句append進去，而不是每句都分開標記成1\n",
    "#             labels.append(1) # 因此每個evidence_set最多只會產生一項標記為1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        evidence_set = set([(evidence[2], evidence[3])\n",
    "                            for evidences in df[\"evidence\"][i]\n",
    "                            for evidence in evidences])\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [\n",
    "                    (page, sent_idx) for sent_idx in mapping[page].keys()\n",
    "                ]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for pair in page_sent_id_pairs:\n",
    "                if pair in evidence_set:\n",
    "                    continue\n",
    "                text = mapping[page][pair[1]]\n",
    "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
    "                if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text)\n",
    "                    labels.append(0)\n",
    "\n",
    "    return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"label\": labels})\n",
    "\n",
    "\n",
    "def pair_with_wiki_sentences_eval(\n",
    "    mapping: Dict[str, Dict[int, str]],\n",
    "    df: pd.DataFrame,\n",
    "    is_testset: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
    "    claims = []\n",
    "    sentences = []\n",
    "    evidence = []\n",
    "    predicted_evidence = []\n",
    "\n",
    "    # negative\n",
    "    for i in range(len(df)):\n",
    "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
    "        #     continue\n",
    "        claim = df[\"claim\"].iloc[i]\n",
    "\n",
    "        predicted_pages = df[\"predicted_pages\"][i]\n",
    "        for page in predicted_pages:\n",
    "            page = page.replace(\" \", \"_\")\n",
    "            try:\n",
    "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
    "            except KeyError:\n",
    "                # print(f\"{page} is not in our Wiki db.\")\n",
    "                continue\n",
    "\n",
    "            for page_name, sentence_id in page_sent_id_pairs:\n",
    "                text = mapping[page][sentence_id]\n",
    "                if text != \"\":\n",
    "                    claims.append(claim)\n",
    "                    sentences.append(text)\n",
    "                    if not is_testset:\n",
    "                        evidence.append(df[\"evidence\"].iloc[i])\n",
    "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"claim\": claims,\n",
    "        \"text\": sentences,\n",
    "        \"evidence\": evidence if not is_testset else None,\n",
    "        \"predicted_evidence\": predicted_evidence,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
    "NUM_EPOCHS = 10  #@param {type:\"integer\"}\n",
    "LR = 2e-5  #@param {type:\"number\"}\n",
    "TRAIN_BATCH_SIZE = 7  #@param {type:\"integer\"}\n",
    "TEST_BATCH_SIZE = 7  #@param {type:\"integer\"}\n",
    "MAX_SEQ_LEN =  512  #@param {type:\"integer\"}\n",
    "NEGATIVE_RATIO = 0.1  #@param {type:\"number\"}\n",
    "VALIDATION_STEP = 250  #@param {type:\"integer\"}\n",
    "TOP_N = 5  #@param {type:\"integer\"}\n",
    "ACCUMULATION_STEP = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ToLvE9oxIXQo"
   },
   "outputs": [],
   "source": [
    "EXP_DIR = f\"sent_retrieval/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_neg{NEGATIVE_RATIO}_top{TOP_N}\"\n",
    "LOG_DIR = \"logs/\" + EXP_DIR\n",
    "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
    "\n",
    "if not Path(LOG_DIR).exists():\n",
    "    Path(LOG_DIR).mkdir(parents=True)\n",
    "\n",
    "if not Path(CKPT_DIR).exists():\n",
    "    Path(CKPT_DIR).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Combine claims and evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4A5vWEzPiXGF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using the following train data with 0 (Negative) and 1 (Positive)\n",
      "0    14752\n",
      "1    12407\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = pair_with_wiki_sentences(\n",
    "    mapping,\n",
    "    pd.DataFrame(TRAIN_GT),\n",
    "    NEGATIVE_RATIO,\n",
    ")\n",
    "counts = train_df[\"label\"].value_counts()\n",
    "print(\"Now using the following train data with 0 (Negative) and 1 (Positive)\")\n",
    "print(counts)\n",
    "\n",
    "dev_evidences = pair_with_wiki_sentences_eval(mapping, pd.DataFrame(DEV_GT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14752\n",
       "1    12407\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "l48WifjeIGui"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = SentRetrievalBERTDataset(train_df, tokenizer=tokenizer , max_length = MAX_SEQ_LEN)\n",
    "val_dataset = SentRetrievalBERTDataset(dev_evidences, tokenizer=tokenizer , max_length = MAX_SEQ_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    ")\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4rl_u0YbeQtY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(LABEL2ID))\n",
    "config.hidden_dropout_prob = 0.3\n",
    "config.attention_probs_dropout_prob = 0.3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
    "\n",
    "writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-chinese\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.3,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.3,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.27.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name = \"model.250.pt\"  #@param {type:\"string\"}\n",
    "model = load_model(model, ckpt_name, CKPT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure that you are using gpu when training (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1AHGaKh1eKmg",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f94bdd1bd347fab5ddb5fedd008fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a341acb51954444d94fbc6ce986c1f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.715997\n",
      "val_results :  {'F1 score': 0.2599856312847489, 'Precision': 0.19674698795180778, 'Recall': 0.38313253012048193}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a0f040e5d8469a9c5f270f3371060b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.688297\n",
      "val_results :  {'F1 score': 0.24922472384639843, 'Precision': 0.1895180722891571, 'Recall': 0.363855421686747}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0a95d088164e18912efc7c9c6c1dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.692031\n",
      "val_results :  {'F1 score': 0.2546600515732914, 'Precision': 0.19192771084337407, 'Recall': 0.3783132530120482}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "current_steps = 0\n",
    "\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "max_precision = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    loss_info = 0\n",
    "\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss / ACCUMULATION_STEP\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % ACCUMULATION_STEP == 0 and  i > 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        loss_info += outputs.loss.item()\n",
    "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
    "        \n",
    "            \n",
    "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        y_true = batch[\"labels\"].tolist()\n",
    "\n",
    "        current_steps += 1\n",
    "\n",
    "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
    "            print(\"Start validation\")\n",
    "\n",
    "            # 計算出dev_evidences每一項的prob (prob為該句是證據句的機率)\n",
    "            probs = get_predicted_probs(model, eval_dataloader, device)\n",
    "            \n",
    "\n",
    "            val_results = evaluate_retrieval(\n",
    "                probs=probs,\n",
    "                df_evidences=dev_evidences,\n",
    "                ground_truths=DEV_GT,\n",
    "                top_n=TOP_N,\n",
    "            )\n",
    "            print(f'train_loss:{loss_info  / VALIDATION_STEP : .6f}')\n",
    "            print('val_results : ' , val_results)\n",
    "            max_precision = max(max_precision , val_results[\"Precision\"])\n",
    "\n",
    "            # log each metric separately to TensorBoard\n",
    "            for metric_name, metric_value in val_results.items():\n",
    "                writer.add_scalar(\n",
    "                    f\"dev_{metric_name}\",\n",
    "                    metric_value,\n",
    "                    current_steps,\n",
    "                )\n",
    "            save_checkpoint(model, CKPT_DIR, current_steps)\n",
    "            loss_info = 0\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs --host 6000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation part (15 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QS1Ei5DAIO5p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start final evaluations and write prediction files.\n",
      "Start calculating training scores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa58328cfbbb4716bc7a7ee9f85962e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores => {'F1 score': 0.4886303511003634, 'Precision': 0.34893140733231437, 'Recall': 0.8148758236188546}\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6400484a9b0b487b8e2867313c0fafa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores => {'F1 score': 0.47526216842486646, 'Precision': 0.3331325301204825, 'Recall': 0.8289156626506025}\n"
     ]
    }
   ],
   "source": [
    "ckpt_name = \"model.600.pt\"  #@param {type:\"string\"}\n",
    "model = load_model(model, ckpt_name, CKPT_DIR)\n",
    "print(\"Start final evaluations and write prediction files.\")\n",
    "\n",
    "train_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping=mapping,\n",
    "    df=pd.DataFrame(TRAIN_GT),\n",
    ")\n",
    "train_set = SentRetrievalBERTDataset(train_evidences, tokenizer, max_length = MAX_SEQ_LEN)\n",
    "train_dataloader = DataLoader(train_set, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "print(\"Start calculating training scores\")\n",
    "probs = get_predicted_probs(model, train_dataloader, device)\n",
    "train_results = evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=train_evidences,\n",
    "    ground_truths=TRAIN_GT,\n",
    "    top_n=TOP_N,\n",
    "    save_name=f\"train_doc5sent{TOP_N}.jsonl\",\n",
    ")\n",
    "print(f\"Training scores => {train_results}\")\n",
    "\n",
    "print(\"Start validation\")\n",
    "probs = get_predicted_probs(model, eval_dataloader, device)\n",
    "val_results = evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=dev_evidences,\n",
    "    ground_truths=DEV_GT,\n",
    "    top_n=TOP_N,\n",
    "    save_name=f\"dev_doc5sent{TOP_N}.jsonl\",\n",
    ")\n",
    "\n",
    "print(f\"Validation scores => {val_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Check on our test data\n",
    "(5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVFusJqjmex-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting the test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2149f7ad9e0453d84e048368b63427a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = load_json(\"data/test_doc5.jsonl\")\n",
    "\n",
    "test_evidences = pair_with_wiki_sentences_eval(\n",
    "    mapping,\n",
    "    pd.DataFrame(test_data),\n",
    "    is_testset=True,\n",
    ")\n",
    "test_set = SentRetrievalBERTDataset(test_evidences, tokenizer, max_length = MAX_SEQ_LEN)\n",
    "test_dataloader = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "print(\"Start predicting the test data\")\n",
    "probs = get_predicted_probs(model, test_dataloader, device)\n",
    "evaluate_retrieval(\n",
    "    probs=probs,\n",
    "    df_evidences=test_evidences,\n",
    "    ground_truths=test_data,\n",
    "    top_n=TOP_N,\n",
    "    cal_scores=False,\n",
    "    save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
