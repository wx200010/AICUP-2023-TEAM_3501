{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzl8a5JteT7"
   },
   "source": [
    "# PART 3. Claim verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tgA1vcUyzjlx"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/train_doc5sent5.jsonl\")\n",
    "DEV_DATA = load_json(\"data/dev_doc5sent5.jsonl\")\n",
    "\n",
    "TRAIN_PKL_FILE = Path(\"data/train_doc5sent5.pkl\")\n",
    "DEV_PKL_FILE = Path(\"data/dev_doc5sent5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>predicted_pages</th>\n",
       "      <th>predicted_evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5475</td>\n",
       "      <td>supports</td>\n",
       "      <td>IIA的執行主席比爾 · 比紹普於2004年過世。</td>\n",
       "      <td>[[[4549, 4635, 比爾·比紹普, 3], [4549, 4635, 比爾·比紹普...</td>\n",
       "      <td>[比爾·比紹普]</td>\n",
       "      <td>[[比爾·比紹普, 0], [比爾·比紹普, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12722</td>\n",
       "      <td>refutes</td>\n",
       "      <td>由爾文·克許納製作的《星際大戰五部曲：帝國大反擊》是續集超越前作的典範，內容是發生在前作《星...</td>\n",
       "      <td>[[[11729, 10428, 星際大戰五部曲：帝國大反擊, 0]]]</td>\n",
       "      <td>[星際大戰_(消歧義), 星際大戰五部曲：帝國大反擊, 爾文·克許納]</td>\n",
       "      <td>[[星際大戰五部曲：帝國大反擊, 18], [爾文·克許納, 0], [星際大戰五部曲：帝國...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1546</td>\n",
       "      <td>supports</td>\n",
       "      <td>上夸克與下垮克是所有夸克質量最低的，質量較高的夸克藉由粒子衰變的自發過程變成上或下夸克。</td>\n",
       "      <td>[[[1488, 1595, 夸克, 8], [1488, 1595, 粒子衰變, 0], ...</td>\n",
       "      <td>[粒子衰變, 上夸克, 夸克, 下夸克]</td>\n",
       "      <td>[[夸克, 7], [夸克, 8], [粒子衰變, 0], [下夸克, 1], [夸克, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7916</td>\n",
       "      <td>refutes</td>\n",
       "      <td>建於1659年的基留希是位於北美洲的一座村落。</td>\n",
       "      <td>[[[7131, 6794, 基留希, 0]]]</td>\n",
       "      <td>[基留希]</td>\n",
       "      <td>[[基留希, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10960</td>\n",
       "      <td>supports</td>\n",
       "      <td>阿登多夫在2011年的人口數據中爲女生多於男生。</td>\n",
       "      <td>[[[10201, 9240, 阿登多夫, 0], [10201, 9240, 阿登多夫, ...</td>\n",
       "      <td>[阿登多夫]</td>\n",
       "      <td>[[阿登多夫, 0], [阿登多夫, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>4947</td>\n",
       "      <td>refutes</td>\n",
       "      <td>藤子·F·不二雄是藤本弘單飛後的匿名。</td>\n",
       "      <td>[[[6796, 6520, 藤子·F·不二雄, 6]]]</td>\n",
       "      <td>[藤子·F·不二雄, 藤子·F·不二雄博物館, 藤子·F·不二雄創作]</td>\n",
       "      <td>[[藤子·F·不二雄, 0], [藤子·F·不二雄博物館, 27], [藤子·F·不二雄創作...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>154</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>大象出現於唐朝。</td>\n",
       "      <td>[[313, None, None, None]]</td>\n",
       "      <td>[黔中郡_(唐朝)]</td>\n",
       "      <td>[[黔中郡_(唐朝), 3], [黔中郡_(唐朝), 0], [黔中郡_(唐朝), 4], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>12531</td>\n",
       "      <td>refutes</td>\n",
       "      <td>在拉脫維亞舉行的2003年歐洲歌唱大賽的冠軍從缺。</td>\n",
       "      <td>[[[11900, 10569, 2003年歐洲歌唱大賽, 20]]]</td>\n",
       "      <td>[2003年歐洲歌唱大賽]</td>\n",
       "      <td>[[2003年歐洲歌唱大賽, 0], [2003年歐洲歌唱大賽, 4], [2003年歐洲歌...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>754</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>剛果民主共和國城市爲金沙薩。</td>\n",
       "      <td>[[871, None, None, None]]</td>\n",
       "      <td>[維維_(剛果民主共和國), 剛果民主共和國]</td>\n",
       "      <td>[[剛果民主共和國, 0], [剛果民主共和國, 9], [剛果民主共和國, 8], [剛果...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>2355</td>\n",
       "      <td>refutes</td>\n",
       "      <td>三苗主要分佈於黃河中下游一帶，戰國時人認爲上古三苗部落位於古洞庭湖與鄱陽湖之間，但也有觀點認...</td>\n",
       "      <td>[[[2655, 2878, 三苗, 3]]]</td>\n",
       "      <td>[三苗]</td>\n",
       "      <td>[[三苗, 3], [三苗, 4], [三苗, 29], [三苗, 14], [三苗, 0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id            label  \\\n",
       "0     5475         supports   \n",
       "1    12722          refutes   \n",
       "2     1546         supports   \n",
       "3     7916          refutes   \n",
       "4    10960         supports   \n",
       "..     ...              ...   \n",
       "576   4947          refutes   \n",
       "577    154  NOT ENOUGH INFO   \n",
       "578  12531          refutes   \n",
       "579    754  NOT ENOUGH INFO   \n",
       "580   2355          refutes   \n",
       "\n",
       "                                                 claim  \\\n",
       "0                            IIA的執行主席比爾 · 比紹普於2004年過世。   \n",
       "1    由爾文·克許納製作的《星際大戰五部曲：帝國大反擊》是續集超越前作的典範，內容是發生在前作《星...   \n",
       "2         上夸克與下垮克是所有夸克質量最低的，質量較高的夸克藉由粒子衰變的自發過程變成上或下夸克。   \n",
       "3                              建於1659年的基留希是位於北美洲的一座村落。   \n",
       "4                             阿登多夫在2011年的人口數據中爲女生多於男生。   \n",
       "..                                                 ...   \n",
       "576                                藤子·F·不二雄是藤本弘單飛後的匿名。   \n",
       "577                                           大象出現於唐朝。   \n",
       "578                          在拉脫維亞舉行的2003年歐洲歌唱大賽的冠軍從缺。   \n",
       "579                                     剛果民主共和國城市爲金沙薩。   \n",
       "580  三苗主要分佈於黃河中下游一帶，戰國時人認爲上古三苗部落位於古洞庭湖與鄱陽湖之間，但也有觀點認...   \n",
       "\n",
       "                                              evidence  \\\n",
       "0    [[[4549, 4635, 比爾·比紹普, 3], [4549, 4635, 比爾·比紹普...   \n",
       "1                 [[[11729, 10428, 星際大戰五部曲：帝國大反擊, 0]]]   \n",
       "2    [[[1488, 1595, 夸克, 8], [1488, 1595, 粒子衰變, 0], ...   \n",
       "3                             [[[7131, 6794, 基留希, 0]]]   \n",
       "4    [[[10201, 9240, 阿登多夫, 0], [10201, 9240, 阿登多夫, ...   \n",
       "..                                                 ...   \n",
       "576                      [[[6796, 6520, 藤子·F·不二雄, 6]]]   \n",
       "577                          [[313, None, None, None]]   \n",
       "578                [[[11900, 10569, 2003年歐洲歌唱大賽, 20]]]   \n",
       "579                          [[871, None, None, None]]   \n",
       "580                            [[[2655, 2878, 三苗, 3]]]   \n",
       "\n",
       "                         predicted_pages  \\\n",
       "0                               [比爾·比紹普]   \n",
       "1    [星際大戰_(消歧義), 星際大戰五部曲：帝國大反擊, 爾文·克許納]   \n",
       "2                   [粒子衰變, 上夸克, 夸克, 下夸克]   \n",
       "3                                  [基留希]   \n",
       "4                                 [阿登多夫]   \n",
       "..                                   ...   \n",
       "576  [藤子·F·不二雄, 藤子·F·不二雄博物館, 藤子·F·不二雄創作]   \n",
       "577                           [黔中郡_(唐朝)]   \n",
       "578                        [2003年歐洲歌唱大賽]   \n",
       "579              [維維_(剛果民主共和國), 剛果民主共和國]   \n",
       "580                                 [三苗]   \n",
       "\n",
       "                                    predicted_evidence  \n",
       "0                           [[比爾·比紹普, 0], [比爾·比紹普, 3]]  \n",
       "1    [[星際大戰五部曲：帝國大反擊, 18], [爾文·克許納, 0], [星際大戰五部曲：帝國...  \n",
       "2     [[夸克, 7], [夸克, 8], [粒子衰變, 0], [下夸克, 1], [夸克, 9]]  \n",
       "3                                           [[基留希, 0]]  \n",
       "4                               [[阿登多夫, 0], [阿登多夫, 1]]  \n",
       "..                                                 ...  \n",
       "576  [[藤子·F·不二雄, 0], [藤子·F·不二雄博物館, 27], [藤子·F·不二雄創作...  \n",
       "577  [[黔中郡_(唐朝), 3], [黔中郡_(唐朝), 0], [黔中郡_(唐朝), 4], ...  \n",
       "578  [[2003年歐洲歌唱大賽, 0], [2003年歐洲歌唱大賽, 4], [2003年歐洲歌...  \n",
       "579  [[剛果民主共和國, 0], [剛果民主共和國, 9], [剛果民主共和國, 8], [剛果...  \n",
       "580    [[三苗, 3], [三苗, 4], [三苗, 29], [三苗, 14], [三苗, 0]]  \n",
       "\n",
       "[581 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(DEV_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mapping_path = Path(f\"data/part2_mapping.json\")\n",
    "if mapping_path.exists():\n",
    "    mapping = json.load( open( \"data/part2_mapping.json\" ) )\n",
    "else:\n",
    "    wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "    mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "    json.dump( mapping, open( \"data/part2_mapping.json\", 'w' ) )\n",
    "    del wiki_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload wiki database (same as part 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AICUP dataset with top-k evidence sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
    "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        claim = item[\"claim\"]\n",
    "        evidence = item[\"evidence_list\"]\n",
    "\n",
    "        # In case there are less than topk evidence sentences\n",
    "        pad = [\"[PAD]\"] * (self.topk - len(evidence))\n",
    "        evidence += pad\n",
    "        concat_claim_evidence = \" [SEP] \".join([*claim, *evidence])\n",
    "#         print(concat_claim_evidence)\n",
    "        concat = self.tokenizer(\n",
    "            concat_claim_evidence,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(label)\n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            y_true.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    if mode == \"eval\":\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
    "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "            for evi_id, evi_idx in x  # for each evidence list\n",
    "        ][:topk] if isinstance(x, list) else [])\n",
    "        print(df[\"evidence_list\"][:5])\n",
    "    else:\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
    "            \" \".join([  # join evidence\n",
    "                mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "                for _, _, evi_id, evi_idx in evi_list\n",
    "            ]) if isinstance(evi_list, list) else \"\"\n",
    "            for evi_list in x  # for each evidence list\n",
    "        ][:1] if isinstance(x, list) else [])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Setup training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  { display-mode: \"form\" }\n",
    "\n",
    "MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
    "TRAIN_BATCH_SIZE = 7  #@param {type:\"integer\"}\n",
    "TEST_BATCH_SIZE = 7  #@param {type:\"integer\"}\n",
    "SEED = 20  #@param {type:\"integer\"}\n",
    "LR = 1e-4  #@param {type:\"number\"}\n",
    "NUM_EPOCHS = 10  #@param {type:\"integer\"}\n",
    "MAX_SEQ_LEN =  512  #@param {type:\"integer\"}\n",
    "EVIDENCE_TOPK = 5  #@param {type:\"integer\"}\n",
    "VALIDATION_STEP = 250  #@param {type:\"integer\"}\n",
    "\n",
    "ACCUMULATION_STEP = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = \"submission.jsonl\"\n",
    "\n",
    "EXP_DIR = f\"claim_verification/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_top{EVIDENCE_TOPK}\"\n",
    "LOG_DIR = \"logs/\" + EXP_DIR\n",
    "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
    "\n",
    "if not Path(LOG_DIR).exists():\n",
    "    Path(LOG_DIR).mkdir(parents=True)\n",
    "\n",
    "if not Path(CKPT_DIR).exists():\n",
    "    Path(CKPT_DIR).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Concat claim and evidences\n",
    "join topk evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_PKL_FILE.exists():\n",
    "    train_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TRAIN_DATA),\n",
    "        mapping,\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    train_df.to_pickle(TRAIN_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(TRAIN_PKL_FILE, \"rb\") as f:\n",
    "        train_df = pickle.load(f)\n",
    "\n",
    "if not DEV_PKL_FILE.exists():\n",
    "    dev_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(DEV_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    dev_df.to_pickle(DEV_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(DEV_PKL_FILE, \"rb\") as f:\n",
    "        dev_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supports           4744\n",
       "refutes            3148\n",
       "NOT ENOUGH INFO    3147\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent CUDA out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "want_index = train_df[train_df[\"label\"]==\"refutes\"].index\n",
    "copy_df = train_df.loc[want_index].sample(frac=0.5, replace=True, random_state=42)\n",
    "train_df = pd.concat([train_df, copy_df] , ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supports           4744\n",
       "refutes            4092\n",
       "NOT ENOUGH INFO    3147\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supports           250\n",
       "NOT ENOUGH INFO    166\n",
       "refutes            165\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "O0rVk3990DlD"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "val_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    ")\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CzMgs-Zs3sTN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(LABEL2ID))\n",
    "config.hidden_dropout_prob = 0.3\n",
    "config.attention_probs_dropout_prob = 0.3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model  = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
    "\n",
    "writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-chinese\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.3,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.3,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.27.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints/claim_verification/e3_bs14_0.0003_top5/val_acc=0.4871_model.1200.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_acc=0.4871_model.1200.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCKPT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Downloads\\AIcup2023\\utils.py:128\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model, ckpt_name, ckpt_dir)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model, ckpt_name, ckpt_dir: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 128\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mckpt_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/claim_verification/e3_bs14_0.0003_top5/val_acc=0.4871_model.1200.pt'"
     ]
    }
   ],
   "source": [
    "model = load_model(model, \"val_acc=0.4871_model.1200.pt\", CKPT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (30 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_aqMjEek3wmu",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecbc4c0d1c545b4894e4faaf0062e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2568 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561907aed2b24ce99967dbcc66aacf2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.354030 , train_loss: 1.135963\n",
      "val_acc: 0.481928 , val_loss: 1.068603\n",
      "\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdfab3f112f409889f3ab7a18ffd4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.463531 , train_loss: 0.847310\n",
      "val_acc: 0.485370 , val_loss: 1.118052\n",
      "\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769c6128283e41c4af9066beebfd5f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.551509 , train_loss: 0.570238\n",
      "val_acc: 0.485370 , val_loss: 1.388660\n",
      "\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc278ffc2c714f8b8584ce6c17494d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.596420 , train_loss: 0.501218\n",
      "val_acc: 0.487091 , val_loss: 1.622366\n",
      "\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b604f1dd22545fc8fa2b6578081d121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.449664 , train_loss: 0.364168\n",
      "val_acc: 0.483649 , val_loss: 1.745520\n",
      "\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5e682f1c8d488e945a2fafc2d81622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.637806 , train_loss: 0.485194\n",
      "val_acc: 0.487091 , val_loss: 1.609402\n",
      "\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3496c60eebf74e4aa57bcb5de75d9515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.647791 , train_loss: 0.495083\n",
      "val_acc: 0.487091 , val_loss: 1.776089\n",
      "\n",
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910fbc7c7d2f4d289d04a3988a39ae43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.658976 , train_loss: 0.469379\n",
      "val_acc: 0.488812 , val_loss: 1.956687\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "current_steps = 0\n",
    "\n",
    "train_info_list = {\"acc\":[] , \"loss\":[]}\n",
    "val_info_list = {\"acc\":[] , \"loss\":[]}\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "max_val_acc = 0\n",
    "accuracy_info = 0\n",
    "loss_info = 0\n",
    "val_loss = 0\n",
    "val_acc = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    accuracy_info = 0\n",
    "    loss_info = 0\n",
    "    \n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # 計算損失\n",
    "        loss =  outputs.loss / ACCUMULATION_STEP\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        if i % ACCUMULATION_STEP == 0 and  i > 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        progress_bar.update(1)\n",
    "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
    "\n",
    "        y_pred.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "        y_true.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "        accuracy_info += accuracy_score(y_pred,y_true)\n",
    "        loss_info += outputs.loss.item()\n",
    "        \n",
    "        current_steps += 1\n",
    "            \n",
    "\n",
    "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
    "            print(\"Start validation\")\n",
    "            val_results = run_evaluation(model, eval_dataloader, device)\n",
    "            val_loss = val_results[\"val_loss\"]\n",
    "            val_acc = val_results[\"val_acc\"]\n",
    "            max_val_acc = max(max_val_acc , val_acc)\n",
    "            \n",
    "            # log each metric separately to TensorBoard\n",
    "            for metric_name, metric_value in val_results.items():\n",
    "                writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
    "                \n",
    "#             if(val_acc >= 0.4 and abs(val_acc-max_val_acc) <= 0.001):\n",
    "            save_checkpoint(\n",
    "                model,\n",
    "                CKPT_DIR,\n",
    "                current_steps,\n",
    "                mark=f\"val_acc={val_results['val_acc']:.4f}\",\n",
    "            )\n",
    "\n",
    "            # 將資訊整理進List\n",
    "            train_info_list[\"acc\"].append(accuracy_info / VALIDATION_STEP)\n",
    "            train_info_list[\"loss\"].append(loss_info  / VALIDATION_STEP)\n",
    "            val_info_list[\"acc\"].append(val_acc)\n",
    "            val_info_list[\"loss\"].append(val_loss)\n",
    "            print(f'train_acc:{accuracy_info / VALIDATION_STEP : .6f} , train_loss:{loss_info  / VALIDATION_STEP : .6f}')\n",
    "            print(f'val_acc:{val_acc : .6f} , val_loss:{val_loss : .6f}')\n",
    "            print()\n",
    "            accuracy_info = 0\n",
    "            loss_info = 0\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model, \"val_acc=0.4888_model.1600.pt\", CKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09029fa76d1406daa2d3816ebde39e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Start validation\")\n",
    "val_results = run_evaluation(model, eval_dataloader, device)\n",
    "val_loss = val_results[\"val_loss\"]\n",
    "val_acc = val_results[\"val_acc\"]\n",
    "max_val_acc = max(max_val_acc , val_acc)\n",
    "\n",
    "# log each metric separately to TensorBoard\n",
    "for metric_name, metric_value in val_results.items():\n",
    "    writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
    "\n",
    "#             if(val_acc >= 0.4 and abs(val_acc-max_val_acc) <= 0.001):\n",
    "save_checkpoint(\n",
    "    model,\n",
    "    CKPT_DIR,\n",
    "    current_steps,\n",
    "    mark=f\"val_acc={val_results['val_acc']:.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_checkpoint(\n",
    "    model,\n",
    "    CKPT_DIR,\n",
    "    current_steps,\n",
    "    mark=f\"val_acc={val_results['val_acc']:.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 16660), started 6 days, 6:01:57 ago. (Use '!kill 16660' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-925d15d18cf2d4f9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-925d15d18cf2d4f9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Make your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "zLkfuoAE49mz"
   },
   "outputs": [],
   "source": [
    "TEST_DATA = load_json(\"data/test_doc5sent5.jsonl\")\n",
    "TEST_PKL_FILE = Path(\"data/test_doc5sent5.pkl\")\n",
    "\n",
    "if not TEST_PKL_FILE.exists():\n",
    "    test_df = join_with_topk_evidence(\n",
    "        pd.DataFrame(TEST_DATA),\n",
    "        mapping,\n",
    "        mode=\"eval\",\n",
    "        topk=EVIDENCE_TOPK,\n",
    "    )\n",
    "    test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n",
    "else:\n",
    "    with open(TEST_PKL_FILE, \"rb\") as f:\n",
    "        test_df = pickle.load(f)\n",
    "\n",
    "test_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = []\n",
    "models.append(load_model(model, \"val_acc=0.4888_model.1600.pt\", CKPT_DIR))\n",
    "# models.append(load_model(model, \"val_acc=0.5611_model.16500.pt\", CKPT_DIR))\n",
    "# models.append(load_model(model, \"val_acc=0.5577_model.17000.pt\", CKPT_DIR))\n",
    "# models.append(load_model(model, \"val_acc=0.5047_model.46000(分數為0.451972).pt\", CKPT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(models: list, test_dl: DataLoader, device) -> list:\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    preds = []\n",
    "    for batch in tqdm(test_dl,\n",
    "                      total=len(test_dl),\n",
    "                      leave=False,\n",
    "                      desc=\"Predicting\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        tmpList = []\n",
    "            \n",
    "        for model in models:\n",
    "            pred = model(**batch).logits\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            tmpList.append(pred.tolist())\n",
    "        FinalPred = []\n",
    "        \n",
    "        for col in range(len(tmpList[0])):\n",
    "            num = [0,0,0]\n",
    "            for row in range(len(tmpList)):\n",
    "                num[tmpList[row][col]] += 1\n",
    "            ans = num.index(max(num))\n",
    "            FinalPred.append(ans)\n",
    "        \n",
    "#         print(tmpList[0])\n",
    "#         print(tmpList[1])\n",
    "#         print(tmpList[2])\n",
    "#         print(FinalPred)\n",
    "#         print('-'*50)\n",
    "        preds.extend(FinalPred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tqIjlht8yCMA"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/646 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_label = run_predict(models, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Gl9I3ZWW4pHo"
   },
   "outputs": [],
   "source": [
    "predict_dataset = test_df.copy()\n",
    "predict_dataset[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label))\n",
    "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
    "    OUTPUT_FILENAME,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_json(r'./submission.jsonl',orient=\"records\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supports           8141\n",
       "NOT ENOUGH INFO     867\n",
       "refutes              30\n",
       "Name: predicted_label, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df['predicted_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINISH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 檢查submission有沒有不符合格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_json(r'./submission.jsonl',orient=\"records\",lines=True)\n",
    "# 檢查supports的部分是否都含有predicted_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查support或refutes的陳述句是否至少帶有一組證據句\n",
    "check_df = sub_df.iloc[sub_df[sub_df['predicted_label'] != 'NOT ENOUGH INFO'].index]\n",
    "for List in check_df['predicted_evidence']:\n",
    "#     print(List)\n",
    "    if(len(List) == 0):\n",
    "        print('[Error] ENOUGH INFO has empty list', List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查NOT ENOUGH INFO標籤的資料是否皆沒有證據句\n",
    "check_df = sub_df.iloc[sub_df[sub_df['predicted_label'] == 'NOT ENOUGH INFO'].index]\n",
    "for i,List in enumerate(check_df['predicted_evidence']):\n",
    "    if(len(List) != 0):\n",
    "        print('[Error] NOT ENOUGH INFO has evidence , index i', i)\n",
    "        print(List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
