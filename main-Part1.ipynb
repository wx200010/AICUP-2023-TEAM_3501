{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7MSEcenjVrL"
   },
   "source": [
    "notebook1\n",
    "## PART 1. Document retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入相關函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 23:30:38.277040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 23:30:38.785799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import json\n",
    "import jieba\n",
    "\n",
    "jieba.set_dictionary(\"dict.txt.big\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "# Adjust the number of workers if you want\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=5)\n",
    "\n",
    "from utils import load_json, jsonl_dir_to_df, generate_evidence_to_wiki_pages_mapping\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas() # for progress_apply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整理中文停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TCSP import read_stopwords_list\n",
    "stopwords = read_stopwords_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data class for type hinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "@dataclass\n",
    "class Claim:\n",
    "    data: str\n",
    "\n",
    "@dataclass\n",
    "class AnnotationID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class EvidenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class PageTitle:\n",
    "    title: str\n",
    "\n",
    "@dataclass\n",
    "class SentenceID:\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class Evidence:\n",
    "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str, stopwords: list) -> str:  \n",
    "    \"\"\"This function performs Chinese word segmentation and removes stopwords.\n",
    "\n",
    "    Args:\n",
    "        text (str): claim or wikipedia article\n",
    "        stopwords (list): common words that contribute little to the meaning of a sentence\n",
    "\n",
    "    Returns:\n",
    "        str: word segments separated by space (e.g. \"我 喜歡 吃 蘋果\")\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = jieba.cut(text) # TODO: Write your code here\n",
    "    tokens = [w for w in tokens if w not in stopwords ]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    "    mode: str = \"train\",\n",
    "    num_pred_doc: int = 5,\n",
    ") -> None:\n",
    "    with open(\n",
    "        f\"data/{mode}_doc{num_pred_doc}.jsonl\",\n",
    "        \"w\",\n",
    "        encoding=\"utf8\",\n",
    "    ) as f:\n",
    "        for i, d in enumerate(data):\n",
    "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    ") -> None:\n",
    "    precision = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        # Extract all ground truth of titles of the wikipedia pages\n",
    "        # evidence[2] refers to the title of the wikipedia page\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        if len(predicted_pages) != 0:\n",
    "            precision += len(hits) / len(predicted_pages)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Macro precision\n",
    "    return precision / count\n",
    "\n",
    "def calculate_recall(\n",
    "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
    "    predictions: pd.Series,\n",
    ") -> None:\n",
    "    recall = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        recall += len(hits) / len(gt_pages)\n",
    "        count += 1\n",
    "\n",
    "    return recall / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取得wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First time running this cell will 34 minutes using Google Colab.\n",
    "\n",
    "wiki_path = \"data/wiki-pages\"\n",
    "wiki_cache = \"wiki\"\n",
    "target_column = \"text\"\n",
    "\n",
    "wiki_cache_path = Path(f\"data/{wiki_cache}.pkl\")\n",
    "if wiki_cache_path.exists():\n",
    "    wiki_pages = pd.read_pickle(wiki_cache_path)\n",
    "else:\n",
    "    # You need to download `wiki-pages.zip` from the AICUP website\n",
    "    wiki_pages = jsonl_dir_to_df(wiki_path)\n",
    "    # wiki_pages are combined into one dataframe, so we need to reset the index\n",
    "    wiki_pages = wiki_pages.reset_index(drop=True)\n",
    "    # tokenize the text and keep the result in a new column `processed_text`\n",
    "    wiki_pages[\"processed_text\"] = wiki_pages[target_column].parallel_apply(\n",
    "        partial(tokenize, stopwords=stopwords)\n",
    "    )\n",
    "    # save the result to a pickle file\n",
    "    wiki_pages.to_pickle(wiki_cache_path, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算出mapping (下面在找出wiki_sentences時會用到)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_path = Path(f\"data/mapping.json\")\n",
    "if mapping_path.exists():\n",
    "    mapping = json.load( open( \"data/mapping.json\" ) )\n",
    "else:\n",
    "    mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
    "    json.dump( mapping, open( \"data/mapping.json\", 'w' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一步：篩選不重要的wiki_pages (用文本長度)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定要篩選出的最小文本長度 (長度低的文本通常不重要，我測試過設定成90效果不錯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wiki_length = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1187751"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_pages = wiki_pages[\n",
    "    wiki_pages['processed_text'].str.len() > min_wiki_length\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831630"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二步：整理出wiki_sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (若已經有產生檔案，則直接跳到下面讀取即可。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"data/wiki_sentence_all_length90_process.pkl\")\n",
    "if path.exists():\n",
    "    wiki_sentences = pd.read_pickle(\"data/wiki_sentence_all_length90_process.pkl\")  \n",
    "else:\n",
    "    data = {'id': [], 'idx': [], 'text': []}\n",
    "    wiki_sentences = []\n",
    "    for i in tqdm(range(len(wiki_pages))):\n",
    "        id = wiki_pages.iloc[i]['id']\n",
    "    #     print(wiki_pages.iloc[i])\n",
    "        for sentence in mapping[id].values():\n",
    "            if(sentence != ''):\n",
    "                dic = {'id':id, 'idx':int(i), 'text':sentence}\n",
    "                wiki_sentences.append(dic)\n",
    "    wiki_sentences = pd.DataFrame(wiki_sentences)\n",
    "    del wiki_pages\n",
    "    wiki_sentences[\"processed_text\"] = wiki_sentences['text'].parallel_apply(\n",
    "        partial(tokenize, stopwords=stopwords)\n",
    "    )\n",
    "    wiki_sentences.to_pickle('data/wiki_sentence_all_length90_process.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3830335"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三步：開始使用wiki_sentences搭配TF-IDF預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先從wiki_sentences中篩掉長度小於min_sentence_length的，跟上面的文本篩選概念一樣，只是變成每句篩選。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sentence_length = 15\n",
    "num_of_samples = 300\n",
    "topk = 12\n",
    "use_idf = True\n",
    "sublinear_tf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wiki_sentences = wiki_sentences[\n",
    "    wiki_sentences['processed_text'].str.len() >= min_sentence_length\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3482970"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試新方法，將訓練資料集納入字典庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84.6 ms, sys: 0 ns, total: 84.6 ms\n",
      "Wall time: 82.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This cell is for your scores on the training set.\n",
    "DOC_DATA = load_json(\"data/public_train.jsonl\")\n",
    "doc_path = f\"data/train_doc5.jsonl\"\n",
    "# Start to encode the corpus with TF-IDF\n",
    "# DOC_DATA = pd.DataFrame(df)\n",
    "\n",
    "# TRAIN_GT, DEV_GT = train_test_split(\n",
    "#     DOC_DATA,\n",
    "#     test_size=0.1,\n",
    "#     random_state=20,\n",
    "#     shuffle=True\n",
    "# )\n",
    "# TRAIN_GT = pd.DataFrame(TRAIN_GT)\n",
    "TRAIN_GT = pd.DataFrame(DOC_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將train json的資料內容整理並納入wiki_sentences中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/wx200010/AIcup2023/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.udad221628068222d71d4ed32de6bae18.cache\n",
      "Loading model cost 0.478 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "for i,row in TRAIN_GT.iterrows():\n",
    "    if(row['label'] == 'NOT ENOUGH INFO'):\n",
    "        continue\n",
    "    wiki_names = []\n",
    "    evidence_sets = row['evidence']\n",
    "    for sets in evidence_sets:\n",
    "        for one_set in sets:\n",
    "            if(one_set[2] not in wiki_names):\n",
    "                wiki_names.append(one_set[2])\n",
    "    \n",
    "    claim = tokenize(row['claim']  , stopwords)\n",
    "    if len(wiki_names) > 0:\n",
    "        for name in wiki_names:\n",
    "            dic = {'id':name, 'idx':int(i), 'text':row['claim'], 'processed_text':claim}\n",
    "            train_list.append(dic)\n",
    "            \n",
    "        \n",
    "    # 整理成dictionary並新增到wiki_sentences中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = pd.concat([wiki_sentences , pd.DataFrame(train_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = wiki_sentences.drop(['idx'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = wiki_sentences.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>劉懷慎</td>\n",
       "      <td>劉懷慎  ， 彭城人 ， 東晉至南朝宋軍事人物 。</td>\n",
       "      <td>劉懷慎       彭城人     東晉 南朝 宋 軍事 人物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>劉懷慎</td>\n",
       "      <td>任參鎮軍將軍事 、 振威將軍 、 彭城內史 。</td>\n",
       "      <td>任參鎮 軍 將軍 事     振威 將軍     彭 城內 史</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>劉懷慎</td>\n",
       "      <td>跟隨劉裕討伐南燕和盧循 ， 加封輔國將軍 。</td>\n",
       "      <td>跟隨 劉裕 討伐 南燕 盧循     加 封輔國 將軍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>劉懷慎</td>\n",
       "      <td>義熙八年 （ 412年 ） ， 以輔國將軍身份兼任監北徐州諸軍事 ， 鎮守彭城 。</td>\n",
       "      <td>義熙 八年     412       以輔國 將軍 身份 兼任 監北 徐州 軍事     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>劉懷慎</td>\n",
       "      <td>九年 （ 413年 ） ， 討平王靈秀 。</td>\n",
       "      <td>九年     413       討平 王靈秀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492459</th>\n",
       "      <td>加拿大國家銀行</td>\n",
       "      <td>加拿大國家銀行是加拿大第六大商業銀行，是具有信用創作功能的金融機構。</td>\n",
       "      <td>加拿大 國家銀行 加拿大 第六 商業銀行 具有 信用 創作 功能 金融機構</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492460</th>\n",
       "      <td>臺中市文化資產處</td>\n",
       "      <td>臺中市文化資產處爲臺中市政府文化局所屬二級機關 ，是臺中市歷史文化資產保護的專責機關，它在臺...</td>\n",
       "      <td>臺中市 文化 資產 處 爲 臺中市 政府 文化局 所屬 二級 機關   臺中市 歷史 文化 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492461</th>\n",
       "      <td>摩納哥</td>\n",
       "      <td>法國南部的摩納哥除了南部海岸線是靠地中海，其他三面皆被法國包圍。</td>\n",
       "      <td>法國 南部 摩納哥 南部 海岸線 地中海 三面 皆 法國 包圍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492462</th>\n",
       "      <td>啓示錄</td>\n",
       "      <td>《啓示錄》有被收錄在新約聖經中。</td>\n",
       "      <td>啓示錄 收錄 新約 聖經</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492463</th>\n",
       "      <td>達拉斯龍屬</td>\n",
       "      <td>達拉斯龍屬生存於白堊紀晚期的南美洲，它目前僅有一個種特納利達拉斯龍。</td>\n",
       "      <td>達拉斯 龍屬 生存 白堊紀 晚期 南美洲 目前 僅有 種 特納 利 達拉斯 龍</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3492464 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  \\\n",
       "0             劉懷慎                          劉懷慎  ， 彭城人 ， 東晉至南朝宋軍事人物 。   \n",
       "1             劉懷慎                            任參鎮軍將軍事 、 振威將軍 、 彭城內史 。   \n",
       "2             劉懷慎                             跟隨劉裕討伐南燕和盧循 ， 加封輔國將軍 。   \n",
       "3             劉懷慎          義熙八年 （ 412年 ） ， 以輔國將軍身份兼任監北徐州諸軍事 ， 鎮守彭城 。   \n",
       "4             劉懷慎                              九年 （ 413年 ） ， 討平王靈秀 。   \n",
       "...           ...                                                ...   \n",
       "3492459   加拿大國家銀行                 加拿大國家銀行是加拿大第六大商業銀行，是具有信用創作功能的金融機構。   \n",
       "3492460  臺中市文化資產處  臺中市文化資產處爲臺中市政府文化局所屬二級機關 ，是臺中市歷史文化資產保護的專責機關，它在臺...   \n",
       "3492461       摩納哥                   法國南部的摩納哥除了南部海岸線是靠地中海，其他三面皆被法國包圍。   \n",
       "3492462       啓示錄                                   《啓示錄》有被收錄在新約聖經中。   \n",
       "3492463     達拉斯龍屬                 達拉斯龍屬生存於白堊紀晚期的南美洲，它目前僅有一個種特納利達拉斯龍。   \n",
       "\n",
       "                                            processed_text  \n",
       "0                        劉懷慎       彭城人     東晉 南朝 宋 軍事 人物    \n",
       "1                        任參鎮 軍 將軍 事     振威 將軍     彭 城內 史    \n",
       "2                            跟隨 劉裕 討伐 南燕 盧循     加 封輔國 將軍    \n",
       "3        義熙 八年     412       以輔國 將軍 身份 兼任 監北 徐州 軍事     ...  \n",
       "4                                九年     413       討平 王靈秀    \n",
       "...                                                    ...  \n",
       "3492459              加拿大 國家銀行 加拿大 第六 商業銀行 具有 信用 創作 功能 金融機構  \n",
       "3492460  臺中市 文化 資產 處 爲 臺中市 政府 文化局 所屬 二級 機關   臺中市 歷史 文化 ...  \n",
       "3492461                    法國 南部 摩納哥 南部 海岸線 地中海 三面 皆 法國 包圍  \n",
       "3492462                                       啓示錄 收錄 新約 聖經  \n",
       "3492463            達拉斯 龍屬 生存 白堊紀 晚期 南美洲 目前 僅有 種 特納 利 達拉斯 龍  \n",
       "\n",
       "[3492464 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = wiki_sentences['processed_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train TF-IDF vectorizer \n",
    "vectorizer = TfidfVectorizer( # TODO: Write your code here\n",
    "    use_idf = use_idf,\n",
    "    sublinear_tf = sublinear_tf,\n",
    "    stop_words = stopwords\n",
    ")\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_docs_sklearn_sentence(\n",
    "    claim: str,\n",
    "    tokenizing_method: callable,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    topk: int,\n",
    ") -> set:\n",
    "    global wiki_sentences\n",
    "    tokens = tokenizing_method(claim)\n",
    "    claim_vector = vectorizer.transform([tokens])\n",
    "    \n",
    "    similarity_scores = cosine_similarity(claim_vector , X)# TODO: Write your code here\n",
    "    \n",
    "    # `similarity_scores` shape: (num_wiki_pages x 1)\n",
    "    similarity_scores = similarity_scores[0, :]  # flatten the array\n",
    "\n",
    "    \n",
    "    # Sort the similarity scores in descending order\n",
    "    sorted_indices = similarity_scores.argsort()[::-1] # TODO: Write your code here\n",
    "    topk_sorted_indices = sorted_indices[:1000]# TODO: Write your code here\n",
    "    results = []\n",
    "    for idx in topk_sorted_indices:\n",
    "        real_id = wiki_sentences.iloc[idx]['id']\n",
    "        if(real_id not in results):\n",
    "            results.append(real_id)\n",
    "            if(len(results) == topk):\n",
    "                break\n",
    "            \n",
    "    exact_matchs = []\n",
    "    # You can find the following code in our AICUP2023 baseline.\n",
    "    # Basically, we check if a result is exactly mentioned in the claim.\n",
    "    Count = 0\n",
    "    for i,result in enumerate(results):\n",
    "        if (\n",
    "            (result in claim)\n",
    "            or (result in claim.replace(\" \", \"\")) # E.g., MS DOS -> MSDOS\n",
    "            or (result.replace(\"·\", \"\") in claim) # E.g., 湯姆·克魯斯 -> 湯姆克魯斯\n",
    "            or (result.replace(\"-\", \"\") in claim) # E.g., X-SAMPA -> XSAMPA\n",
    "        ):\n",
    "            exact_matchs.append(result)\n",
    "        elif \"·\" in result:\n",
    "            splitted = result.split(\"·\") # E.g., 阿爾伯特·愛因斯坦 -> 愛因斯坦\n",
    "            for split in splitted:\n",
    "                if split in claim:\n",
    "                    exact_matchs.append(result)\n",
    "                    break\n",
    "        elif \"_(\" in result:\n",
    "            splitted = result.split(\"_(\") # E.g., 澎湖_(消歧義) -> 澎湖、消歧異\n",
    "            splitted[1] = splitted[1][:-1]\n",
    "            \n",
    "            for split in splitted:\n",
    "                if(\")\" in split):\n",
    "                    split = split[:-1]\n",
    "                    \n",
    "                if split in claim:\n",
    "                    exact_matchs.append(result)\n",
    "                    break\n",
    "            \n",
    "#     print(set(exact_matchs))\n",
    "    return set(exact_matchs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始對訓練資料找出預測的pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256f4aba04024b8d89627a693b24dbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2324), Label(value='0 / 2324'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 s, sys: 3.07 s, total: 21.5 s\n",
      "Wall time: 1h 16min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This cell is for your scores on the training set.\n",
    "train = load_json(\"data/public_train.jsonl\")\n",
    "doc_path = f\"data/train_doc5.jsonl\"\n",
    "\n",
    "# Start to encode the corpus with TF-IDF\n",
    "train_df = pd.DataFrame(train)\n",
    "\n",
    "# Perform the prediction for document retrieval\n",
    "train_df[\"predicted_pages\"] = train_df[\"claim\"].parallel_apply(\n",
    "    partial(\n",
    "        get_pred_docs_sklearn_sentence,\n",
    "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
    "        vectorizer=vectorizer,\n",
    "        topk=topk,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 印出precision與recall結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topk': 12, 'num_of_samples': 300, 'min_sentence_length': 15, 'precision': 0.6796583915128466, 'recall': 0.9049777296256171}\n"
     ]
    }
   ],
   "source": [
    "precision = calculate_precision(train, train_df[\"predicted_pages\"])\n",
    "recall = calculate_recall(train, train_df[\"predicted_pages\"])\n",
    "dictionary = {'topk':topk,\n",
    "              'num_of_samples':num_of_samples,\n",
    "              'min_sentence_length':min_sentence_length,\n",
    "              'precision':precision,\n",
    "              'recall':recall}\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 儲存train資料的執行結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_doc(train, train_df[\"predicted_pages\"], mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最後一步：對test資料集進行預測pages，並將結果寫入檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceffd9c2c42c4d6ba3daa24667082fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1808), Label(value='0 / 1808'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_path = f\"data/test_doc5.jsonl\"\n",
    "test = load_json(\"data/all_test_data.jsonl\")\n",
    "\n",
    "test_df = pd.DataFrame(test)\n",
    "# Perform the prediction for document retrieval\n",
    "test_df[\"predicted_pages\"] = test_df[\"claim\"].parallel_apply(\n",
    "    partial(\n",
    "        get_pred_docs_sklearn_sentence,\n",
    "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
    "        vectorizer=vectorizer,\n",
    "        topk=topk,\n",
    "    )\n",
    ")\n",
    "save_doc(test, test_df[\"predicted_pages\"], mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
